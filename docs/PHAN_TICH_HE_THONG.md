# PHÃ‚N TÃCH Há»† THá»NG - AI MODEL GIÃM SÃT Máº¬T Äá»˜ GIAO THÃ”NG

## ğŸ“‹ Tá»”NG QUAN CHUNG

ÄÃ¢y lÃ  má»™t há»‡ thá»‘ng AI computer vision sá»­ dá»¥ng **YOLO (You Only Look Once)** Ä‘á»ƒ phÃ¡t hiá»‡n vÃ  Ä‘áº¿m phÆ°Æ¡ng tiá»‡n giao thÃ´ng theo thá»i gian thá»±c. Há»‡ thá»‘ng tá»± Ä‘á»™ng gá»­i cáº£nh bÃ¡o khi máº­t Ä‘á»™ giao thÃ´ng vÆ°á»£t ngÆ°á»¡ng vÃ  tÃ­ch há»£p vá»›i há»‡ thá»‘ng giÃ¡m sÃ¡t lÅ© lá»¥t/táº¯c Ä‘Æ°á»ng thÃ´ng qua API.

### TÃªn Dá»± Ãn
- **Component**: Traffic Density Monitoring System
- **Model**: YOLO v8/v11
- **License**: Apache License 2.0, CC BY 4.0 (dataset)

### Má»¥c ÄÃ­ch
- PhÃ¡t hiá»‡n vÃ  Ä‘áº¿m phÆ°Æ¡ng tiá»‡n giao thÃ´ng tá»« camera/video
- GiÃ¡m sÃ¡t máº­t Ä‘á»™ giao thÃ´ng theo thá»i gian thá»±c
- Tá»± Ä‘á»™ng gá»­i cáº£nh bÃ¡o khi vÆ°á»£t ngÆ°á»¡ng
- TÃ­ch há»£p vá»›i há»‡ thá»‘ng sensor Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o zones cáº£nh bÃ¡o
- Há»— trá»£ nhiá»u loáº¡i phÆ°Æ¡ng tiá»‡n (8 classes)

---

## ğŸ—ï¸ Cáº¤U TRÃšC Dá»° ÃN

```
models/
â”œâ”€â”€ train.py                              # Script huáº¥n luyá»‡n model YOLO
â”œâ”€â”€ traffic_monitor.py                    # Main monitoring application
â”œâ”€â”€ api_client.py                         # API client tÃ­ch há»£p vá»›i backend
â”œâ”€â”€ monitor_config.yaml                   # Cáº¥u hÃ¬nh giÃ¡m sÃ¡t
â”œâ”€â”€ data.yaml                            # Cáº¥u hÃ¬nh dataset cho training
â”‚
â”œâ”€â”€ yolo11n.pt                           # YOLO v11 nano pre-trained (5.4MB)
â”œâ”€â”€ yolov8s.pt                           # YOLO v8 small pre-trained (22MB)
â”œâ”€â”€ calibration_image_sample_data_*.npy  # Sample data cho calibration
â”‚
â”œâ”€â”€ dataset/                             # Dataset YOLOv11 format
â”‚   â”œâ”€â”€ data.yaml                       # Metadata dataset
â”‚   â”œâ”€â”€ README.roboflow.txt             # ThÃ´ng tin tá»« Roboflow
â”‚   â”œâ”€â”€ train/                          # 2,363 áº£nh training
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ labels/                     # YOLO format annotations
â”‚   â”œâ”€â”€ valid/                          # 675 áº£nh validation
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ labels/
â”‚   â””â”€â”€ test/                           # 338 áº£nh testing
â”‚       â”œâ”€â”€ images/
â”‚       â””â”€â”€ labels/
â”‚
â”œâ”€â”€ runs/                                # Training results
â”‚   â””â”€â”€ detect/
â”‚       â”œâ”€â”€ train/                      # First training run
â”‚       â”‚   â”œâ”€â”€ weights/
â”‚       â”‚   â”‚   â”œâ”€â”€ best.pt            # Best model weights
â”‚       â”‚   â”‚   â””â”€â”€ last.pt            # Last checkpoint
â”‚       â”‚   â”œâ”€â”€ results.png            # Training metrics plot
â”‚       â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚       â”‚   â””â”€â”€ ...                    # Other metrics/plots
â”‚       â””â”€â”€ train2/                     # Second training run
â”‚
â”œâ”€â”€ detections/                          # Saved detection images (runtime)
â”œâ”€â”€ output_videos/                       # Saved output videos (optional)
â”œâ”€â”€ monitoring.log                       # Application logs
â”‚
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ LICENSE                              # Apache License 2.0
â””â”€â”€ NOTICE                               # Copyright notice
```

---

## ğŸ’» CÃ”NG NGHá»† Sá»¬ Dá»¤NG

### AI/ML Framework
- **Ultralytics YOLO** (â‰¥8.0.0) - State-of-the-art object detection
  - YOLOv8s: Small model, cÃ¢n báº±ng speed/accuracy
  - YOLOv11n: Nano model, ultra-fast cho edge devices
- **PyTorch** - Deep learning backend (optional, trong ultralytics)

### Computer Vision
- **OpenCV** (â‰¥4.8.0) - Image/video processing
- **NumPy** (â‰¥1.24.0) - Numerical computations
- **Pillow** (â‰¥10.0.0) - Image manipulation

### Integration
- **Requests** (â‰¥2.31.0) - HTTP client cho API calls
- **PyYAML** (â‰¥6.0) - Configuration management

### Hardware Support
- **CPU**: Full support
- **GPU**: CUDA support (optional, faster inference)
  - torch â‰¥2.0.0
  - torchvision â‰¥0.15.0

---

## ğŸ—„ï¸ Cáº¤U TRÃšC Dá»® LIá»†U

### 1. Dataset Configuration (data.yaml)

```yaml
train: dataset/train/images         # 2,363 images
val: dataset/valid/images           # 675 images  
test: dataset/test/images           # 338 images

nc: 8                               # Number of classes
names: [                            # Class names
  'bicycle', 
  'bus', 
  'car', 
  'container_truck', 
  'fire_engine', 
  'motorcycle', 
  'truck', 
  'van'
]

# Dataset info
roboflow:
  workspace: luong-duc
  project: vehicle_detection_project-8jikm
  version: 1
  license: CC BY 4.0
  url: https://universe.roboflow.com/luong-duc/vehicle_detection_project-8jikm/dataset/1
```

**Dataset Details:**
- **Total**: 3,376 images
- **Format**: YOLOv11 (YOLO format)
- **Resolution**: 640x640 (stretched from original)
- **Annotations**: Bounding boxes for 8 vehicle types
- **Pre-processing**: Auto-orientation, resize to 640x640
- **Augmentation**: None (raw dataset)
- **Source**: Roboflow Universe

### 2. YOLO Label Format

Má»—i file `.txt` trong `labels/` cÃ³ format:
```
<class_id> <x_center> <y_center> <width> <height>
```

Táº¥t cáº£ giÃ¡ trá»‹ Ä‘Æ°á»£c normalize (0.0 - 1.0):
- `class_id`: 0-7 (tÆ°Æ¡ng á»©ng vá»›i 8 classes)
- `x_center`, `y_center`: Tá»a Ä‘á»™ tÃ¢m bounding box
- `width`, `height`: KÃ­ch thÆ°á»›c bounding box

**VÃ­ dá»¥:**
```
2 0.5124 0.3456 0.1234 0.2345    # car
5 0.7890 0.6543 0.0987 0.1543    # motorcycle
```

### 3. Monitor Configuration (monitor_config.yaml)

```yaml
api:
  endpoint: "http://localhost:3000/api/sensor-data"
  timeout: 10
  retry_attempts: 3

locations:
  - id: "sensor-intersection-01"
    name: "Main Street Intersection"
    coordinates:
      lat: 10.762622
      lon: 106.660172
    density_threshold: 15           # Vehicles count to trigger alert
    video_source: 0                 # Camera ID or video path
    confidence_threshold: 0.5       # Detection confidence
    check_interval: 30              # Seconds between checks
    count_vehicle_types: [...]      # Which types to count

model:
  weights: "runs/detect/train/weights/best.pt"
  imgsz: 640
  device: 0                         # 0=GPU, 'cpu'=CPU
  half: false                       # FP16 inference
  max_det: 300                      # Max detections per frame

logging:
  level: "INFO"
  save_detections: true
  detections_dir: "detections"
  log_file: "monitoring.log"

display:
  show_video: true                  # Show live preview
  save_video: false
  output_dir: "output_videos"
```

### 4. API Request/Response Format

**Request to Backend API (POST /api/sensor-data):**
```json
{
  "sensorId": "sensor-intersection-01",
  "value": 25.0,                    // Vehicle count
  "timestamp": 1704556789000        // Unix timestamp (ms)
}
```

**Response:**
```json
{
  "success": true,
  "thresholdExceeded": true,
  "sensor": {
    "id": "sensor-intersection-01",
    "threshold": 20
  },
  "automation": {
    "rulesChecked": 3,
    "rulesTriggered": 2,
    "zonesCreated": [
      "auto-zone-1234567890",
      "auto-zone-1234567891"
    ]
  }
}
```

---

## ğŸ”„ LUá»’NG HOáº T Äá»˜NG

### A. Luá»“ng Training Model

1. **Chuáº©n bá»‹ Dataset**
   - Download tá»« Roboflow (3,376 images)
   - Extract vÃ o thÆ° má»¥c `dataset/`
   - Verify structure: train/valid/test vá»›i images + labels

2. **Cáº¥u hÃ¬nh Training**
   - Edit `data.yaml` náº¿u cáº§n (paths, classes)
   - Chá»n pre-trained model: yolov8s.pt hoáº·c yolo11n.pt

3. **Cháº¡y Training**
   ```bash
   python train.py
   ```
   
   **Script thá»±c hiá»‡n:**
   ```python
   model = YOLO("yolov8s.pt")
   model.train(
       data="data.yaml",
       epochs=50,
       imgsz=640,
       batch=16,
       device=0  # GPU
   )
   ```

4. **Training Process**
   - Ultralytics YOLO tá»± Ä‘á»™ng:
     - Load pre-trained weights
     - Fine-tune trÃªn custom dataset
     - Save checkpoints má»—i epoch
     - Generate metrics: mAP, precision, recall
     - Create plots: confusion matrix, F1 curve, etc.

5. **Káº¿t quáº£**
   - LÆ°u trong `runs/detect/train*/`
   - `weights/best.pt`: Model tá»‘t nháº¥t (theo mAP)
   - `weights/last.pt`: Checkpoint cuá»‘i cÃ¹ng
   - `results.png`: Training curves
   - `confusion_matrix.png`: Confusion matrix

6. **Validation**
   - Tá»± Ä‘á»™ng validate trÃªn validation set
   - Metrics: mAP@0.5, mAP@0.5:0.95, precision, recall
   - Class-wise performance

### B. Luá»“ng Monitoring Real-time

1. **Khá»Ÿi Ä‘á»™ng System**
   ```bash
   python traffic_monitor.py --config monitor_config.yaml --location 0
   ```

2. **Initialization (TrafficDensityMonitor.__init__)**
   - Load configuration tá»« YAML
   - Setup logging (console + file)
   - Load trained YOLO model tá»« weights
   - Initialize API client vá»›i retry logic
   - Create output directories

3. **Má»Ÿ Video Source**
   - Camera: `video_source: 0` (webcam)
   - Video file: `video_source: "path/to/video.mp4"`
   - RTSP stream: `video_source: "rtsp://..."`
   - HTTP stream: `video_source: "http://..."`

4. **Main Loop (monitor_location)**
   ```
   WHILE video is active:
     â”œâ”€ Read frame
     â”‚
     â”œâ”€ IF check_interval elapsed:
     â”‚  â”œâ”€ Run YOLO inference
     â”‚  â”œâ”€ Count vehicles by type
     â”‚  â”œâ”€ Calculate total count
     â”‚  â”‚
     â”‚  â”œâ”€ IF count >= density_threshold:
     â”‚  â”‚  â”œâ”€ Log HIGH DENSITY ALERT
     â”‚  â”‚  â”œâ”€ Save detection image
     â”‚  â”‚  â”œâ”€ Send data to API (api_client.send_sensor_data)
     â”‚  â”‚  â”œâ”€ Check API response
     â”‚  â”‚  â””â”€ Log automation triggers
     â”‚  â”‚
     â”‚  â””â”€ ELSE:
     â”‚     â””â”€ Log normal density
     â”‚
     â”œâ”€ IF display.show_video:
     â”‚  â”œâ”€ Run inference for visualization
     â”‚  â”œâ”€ Draw bounding boxes + labels
     â”‚  â”œâ”€ Add text overlay (location name, etc.)
     â”‚  â””â”€ Display in OpenCV window
     â”‚
     â””â”€ Check for quit key ('q')
   ```

5. **YOLO Inference Details**
   ```python
   results = model.predict(
       frame,
       conf=0.5,           # Confidence threshold
       imgsz=640,          # Input size
       device=0,           # GPU
       max_det=300,        # Max detections
       half=False,         # FP16
       verbose=False
   )
   ```

   **Output:**
   - Bounding boxes: [x1, y1, x2, y2]
   - Classes: [0-7]
   - Confidence scores: [0.0-1.0]

6. **Vehicle Counting**
   ```python
   def count_vehicles(results, vehicle_types):
       counts = {vtype: 0 for vtype in vehicle_types}
       for result in results:
           for box in result.boxes:
               class_name = result.names[int(box.cls[0])]
               if class_name in vehicle_types:
                   counts[class_name] += 1
       return counts
   ```

7. **Alert Triggering**
   - Náº¿u `total_count >= density_threshold`:
     - Save áº£nh cÃ³ annotations vÃ o `detections/`
     - Gá»i API vá»›i sensor data
     - Log káº¿t quáº£ automation (zones created)

8. **API Integration**
   - POST request vá»›i retry (3 attempts)
   - Exponential backoff: 2, 4, 8 seconds
   - Handle responses:
     - 200/201: Success
     - 202: Sensor not found (warning)
     - 400: Bad request (error)
     - 5xx: Server error (retry)

### C. Luá»“ng TÃ­ch Há»£p vá»›i Backend

1. **Sensor Registration** (Manual via admin panel)
   - Admin táº¡o sensor trong app
   - Sensor ID pháº£i match vá»›i `id` trong monitor_config.yaml
   - Set threshold cho sensor (vd: 20 vehicles)

2. **Real-time Data Flow**
   ```
   [Camera/Video]
        â†“
   [YOLO Model] â†’ Detect & Count
        â†“
   [Traffic Monitor] â†’ Check threshold
        â†“ (if exceeded)
   [API Client] â†’ POST /api/sensor-data
        â†“
   [Backend Rule Engine] â†’ Check rules
        â†“ (if triggered)
   [Auto Create Zones] â†’ Flood/Outage zones
        â†“
   [WebSocket Broadcast] â†’ Notify all clients
        â†“
   [Map Display] â†’ Show new zones real-time
   ```

3. **Automation Rules**
   - Backend cÃ³ sensor rules (1-sensor, 2-sensor)
   - Khi nháº­n data, rule engine kiá»ƒm tra:
     - Value > threshold?
     - Multiple sensors vá»›i AND/OR logic?
   - Náº¿u trigger â†’ Tá»± Ä‘á»™ng táº¡o zone vá»›i:
     - Type: flood hoáº·c outage
     - Shape: circle (radius tá»« sensor location)
     - Title: "Tá»± Äá»™ng: [Rule Name]"
     - Description: Ghi rÃµ sensor ID vÃ  giÃ¡ trá»‹

### D. Luá»“ng Debugging & Visualization

1. **Live Video Display** (náº¿u `show_video: true`)
   - OpenCV window hiá»ƒn thá»‹ frame vá»›i:
     - Bounding boxes mÃ u (theo class)
     - Class labels + confidence scores
     - Location name overlay
     - Press 'q' Ä‘á»ƒ quit

2. **Save Detections** (náº¿u `save_detections: true`)
   - Má»—i alert táº¡o file áº£nh:
   - Format: `{location_id}_{timestamp}_count{N}.jpg`
   - VÃ­ dá»¥: `sensor-intersection-01_20250106_143052_count25.jpg`

3. **Logging**
   - Console output: Colored, structured
   - File log: `monitoring.log`
   - Log levels: DEBUG, INFO, WARNING, ERROR
   - Ná»™i dung:
     - Frame analysis results
     - Vehicle counts by type
     - Threshold checks
     - API calls and responses
     - Automation triggers

---

## ğŸ“Š METRICS & PERFORMANCE

### Training Metrics

**TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡:**
- **mAP@0.5**: Mean Average Precision vá»›i IoU threshold 0.5
- **mAP@0.5:0.95**: mAP trung bÃ¬nh tá»« IoU 0.5 Ä‘áº¿n 0.95
- **Precision**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **F1 Score**: Harmonic mean cá»§a Precision vÃ  Recall

**Expected Performance** (YOLOv8s on this dataset):
- Training time: ~2-4 hours (GPU)
- mAP@0.5: ~85-95%
- Inference speed: ~50-100 FPS (GPU), ~5-10 FPS (CPU)

### Runtime Performance

**Hardware Requirements:**
- **Minimum (CPU)**:
  - Intel i5 hoáº·c equivalent
  - 8GB RAM
  - 2GB storage cho model + dataset
  - Speed: ~5-10 FPS

- **Recommended (GPU)**:
  - NVIDIA GPU vá»›i CUDA (GTX 1060+)
  - 16GB RAM
  - 4GB storage
  - Speed: ~50-100 FPS

**Optimization Tips:**
- Use YOLOv11n (5.4MB) thay vÃ¬ YOLOv8s (22MB) cho edge devices
- Enable FP16 inference: `half: true` (2x faster on compatible GPUs)
- Giáº£m `imgsz` xuá»‘ng 416 hoáº·c 320 (trade-off accuracy)
- Giáº£m `max_det` náº¿u khÃ´ng cáº§n detect nhiá»u objects
- Batch processing náº¿u process nhiá»u cameras

---

## ğŸ¨ CLASSES & DETECTION

### Vehicle Classes (8 loáº¡i)

| ID | Class Name      | MÃ´ Táº£                     | Typical Size | Priority |
|----|-----------------|---------------------------|--------------|----------|
| 0  | bicycle         | Xe Ä‘áº¡p                    | Small        | Low      |
| 1  | bus             | Xe buÃ½t, xe khÃ¡ch         | Large        | High     |
| 2  | car             | Ã” tÃ´ con, sedan, SUV      | Medium       | High     |
| 3  | container_truck | Xe container              | Very Large   | High     |
| 4  | fire_engine     | Xe cá»©u há»a                | Large        | Critical |
| 5  | motorcycle      | Xe mÃ¡y, mÃ´ tÃ´             | Small        | Medium   |
| 6  | truck           | Xe táº£i                    | Large        | High     |
| 7  | van             | Xe van, minibus           | Medium       | Medium   |

### Detection Examples

**Typical Confidence Scores:**
- Car, Motorcycle: 0.85-0.95 (high accuracy)
- Bus, Truck: 0.75-0.90
- Bicycle, Van: 0.65-0.85
- Container truck, Fire engine: 0.70-0.85

**Challenging Scenarios:**
- Occlusion (xe che khuáº¥t nhau): giáº£m recall
- Night time: giáº£m confidence
- Bad weather (mÆ°a, sÆ°Æ¡ng mÃ¹): giáº£m accuracy
- Small objects (xa camera): giáº£m detection rate
- Motion blur: giáº£m confidence

---

## ğŸ”§ CÃC FILE CHÃNH

### 1. train.py

**Má»¥c Ä‘Ã­ch**: Script train YOLO model trÃªn custom dataset

**Code chÃ­nh:**
```python
from ultralytics import YOLO

model = YOLO("yolov8s.pt")  # Load pre-trained
model.train(
    data="data.yaml",       # Dataset config
    epochs=50,              # Training epochs
    imgsz=640,              # Image size
    batch=16,               # Batch size
    device=0                # GPU device
)
```

**Output**:
- Tá»± Ä‘á»™ng save vÃ o `runs/detect/train/`
- Best model: `weights/best.pt`
- Metrics plots, confusion matrix

### 2. traffic_monitor.py (359 lines)

**Class: TrafficDensityMonitor**

**Methods:**
- `__init__(config_path)`: Khá»Ÿi táº¡o, load model
- `_setup_logging()`: Config logging
- `count_vehicles(results, vehicle_types)`: Äáº¿m xe theo loáº¡i
- `process_frame(frame, location_config)`: Xá»­ lÃ½ 1 frame
- `save_detection_image(results, location_id, count)`: LÆ°u áº£nh
- `monitor_location(location_config)`: Main monitoring loop
- `run(location_index)`: Cháº¡y 1 location
- `run_all_locations()`: Cháº¡y táº¥t cáº£ locations

**Main Entry:**
```python
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="monitor_config.yaml")
    parser.add_argument("--location", type=int, default=0)
    parser.add_argument("--all", action="store_true")
    
    monitor = TrafficDensityMonitor(config_path=args.config)
    
    if args.all:
        monitor.run_all_locations()
    else:
        monitor.run(location_index=args.location)
```

**Usage:**
```bash
# Monitor location 0
python traffic_monitor.py

# Monitor location 1
python traffic_monitor.py --location 1

# Monitor all locations
python traffic_monitor.py --all

# Custom config
python traffic_monitor.py --config my_config.yaml
```

### 3. api_client.py (167 lines)

**Class: SensorAPIClient**

**Methods:**
- `__init__(endpoint, timeout, retry_attempts)`: Khá»Ÿi táº¡o client
- `send_sensor_data(sensor_id, value, timestamp)`: Gá»­i data
- `get_recent_data(limit)`: Láº¥y historical data

**Features:**
- Automatic retry vá»›i exponential backoff
- Comprehensive error handling
- Response parsing and logging
- Support for different status codes

**Usage:**
```python
client = SensorAPIClient(
    endpoint="http://localhost:3000/api/sensor-data",
    timeout=10,
    retry_attempts=3
)

response = client.send_sensor_data(
    sensor_id="sensor-001",
    value=25.5
)
```

---

## ğŸš€ HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG

### Setup Environment

```bash
cd models/

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# For GPU support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Training Workflow

```bash
# 1. Verify dataset structure
ls dataset/train/images  # Should have images
ls dataset/train/labels  # Should have .txt files

# 2. Check data.yaml
cat data.yaml

# 3. Start training
python train.py

# 4. Monitor progress (Ultralytics provides live updates)
# Training metrics displayed in terminal

# 5. Check results
cd runs/detect/train/
ls weights/  # best.pt, last.pt
ls *.png     # Metrics plots

# 6. Validate
python -c "from ultralytics import YOLO; \
           model = YOLO('runs/detect/train/weights/best.pt'); \
           metrics = model.val(); \
           print(metrics)"
```

### Monitoring Workflow

```bash
# 1. Edit configuration
nano monitor_config.yaml

# Key settings to change:
# - api.endpoint: Your API URL
# - locations[0].id: Match vá»›i sensor ID trong backend
# - locations[0].video_source: Camera ID hoáº·c video path
# - locations[0].density_threshold: Alert threshold
# - model.weights: Path to trained model

# 2. Test API connection
python api_client.py

# 3. Start monitoring
python traffic_monitor.py --location 0

# 4. Monitor logs
tail -f monitoring.log

# 5. Check detections
ls detections/
```

### Testing with Video File

```yaml
# monitor_config.yaml
locations:
  - id: "sensor-test-001"
    video_source: "test_video.mp4"  # Path to video
    # ... other settings
```

```bash
python traffic_monitor.py --location 0
```

### Multi-Location Monitoring

**Option 1: Sequential (trong cÃ¹ng 1 process)**
```bash
python traffic_monitor.py --all
```

**Option 2: Parallel (nhiá»u processes)**
```bash
# Terminal 1
python traffic_monitor.py --location 0

# Terminal 2
python traffic_monitor.py --location 1

# Terminal 3
python traffic_monitor.py --location 2
```

---

## ğŸ“¡ API INTEGRATION

### Backend API Requirements

**Endpoint: POST /api/sensor-data**

**Request:**
```json
{
  "sensorId": "string",
  "value": "number",
  "timestamp": "number (optional)"
}
```

**Response:**
```json
{
  "success": "boolean",
  "message": "string (optional)",
  "thresholdExceeded": "boolean (optional)",
  "sensor": {
    "id": "string",
    "name": "string",
    "threshold": "number"
  },
  "automation": {
    "rulesChecked": "number",
    "rulesTriggered": "number",
    "zonesCreated": ["string array"]
  }
}
```

### Error Handling

**Status Codes:**
- 200/201: Success
- 202: Accepted but sensor not found
- 400: Bad request (invalid format)
- 500: Server error
- Timeout: Network error

**Retry Logic:**
- 3 attempts máº·c Ä‘á»‹nh
- Exponential backoff: 2s, 4s, 8s
- Log má»—i attempt
- Raise exception sau khi háº¿t attempts

---

## ğŸ” TROUBLESHOOTING

### Common Issues

**1. Model khÃ´ng load Ä‘Æ°á»£c**
```
Error: No such file or directory: 'runs/detect/train/weights/best.pt'
```
**Solution**: Train model trÆ°á»›c hoáº·c dÃ¹ng pre-trained:
```yaml
model:
  weights: "yolov8s.pt"  # Use pre-trained
```

**2. Camera khÃ´ng má»Ÿ Ä‘Æ°á»£c**
```
Error: Failed to open video source: 0
```
**Solutions:**
- Check camera permissions
- Try different camera ID (0, 1, 2...)
- Check camera Ä‘Ã£ Ä‘Æ°á»£c plug in chÆ°a
- Linux: `ls /dev/video*`

**3. API connection failed**
```
Error: Failed to send data after all retries
```
**Solutions:**
- Check backend server Ä‘ang cháº¡y
- Verify endpoint URL trong config
- Check firewall/network
- Test vá»›i curl:
  ```bash
  curl -X POST http://localhost:3000/api/sensor-data \
    -H "Content-Type: application/json" \
    -d '{"sensorId":"test","value":10}'
  ```

**4. Inference quÃ¡ cháº­m (CPU)**
```
Speed: 0.5 FPS
```
**Solutions:**
- Use smaller model: yolo11n.pt
- Giáº£m imgsz: 320 or 416
- TÄƒng check_interval: 60 seconds
- Enable half precision (GPU only)

**5. Low detection accuracy**
```
Many false positives/negatives
```
**Solutions:**
- TÄƒng confidence_threshold (0.5 â†’ 0.7)
- Re-train vá»›i more data
- Check lighting conditions
- Adjust camera angle

**6. Out of memory (GPU)**
```
CUDA out of memory
```
**Solutions:**
- Giáº£m batch size trong training
- Use smaller model
- Giáº£m imgsz
- Close other GPU applications

---

## ğŸ“ˆ PERFORMANCE OPTIMIZATION

### Inference Speed

**GPU Optimization:**
```yaml
model:
  device: 0        # Use GPU 0
  half: true       # FP16 (2x faster)
  imgsz: 640       # Standard size
  max_det: 300     # Reasonable max
```

**CPU Optimization:**
```yaml
model:
  device: 'cpu'
  half: false      # CPU doesn't support FP16
  imgsz: 416       # Smaller = faster
  max_det: 100     # Limit detections
```

### Memory Usage

**Reduce Memory:**
- Use yolo11n.pt (5.4MB) instead of yolov8s.pt (22MB)
- Lower imgsz: 320, 416
- Disable save_detections if not needed
- Don't show_video in production

### Accuracy vs Speed Trade-off

| Model    | Size  | Speed (GPU) | mAP@0.5 | Use Case          |
|----------|-------|-------------|---------|-------------------|
| YOLO11n  | 5.4MB | ~200 FPS    | ~75%    | Edge, Real-time   |
| YOLO8s   | 22MB  | ~100 FPS    | ~85%    | General           |
| YOLO8m   | 50MB  | ~60 FPS     | ~90%    | High accuracy     |
| YOLO8l   | 88MB  | ~40 FPS     | ~92%    | Research          |
| YOLO8x   | 136MB | ~25 FPS     | ~93%    | Best accuracy     |

---

## ğŸ¯ USE CASES & SCENARIOS

### 1. GiÃ¡m SÃ¡t NgÃ£ TÆ° Giao ThÃ´ng

**Setup:**
```yaml
locations:
  - id: "sensor-intersection-hoangdieu-trahmieu"
    name: "NgÃ£ TÆ° HoÃ ng Diá»‡u - Tráº§n HÆ°ng Miáº¿u"
    coordinates: {lat: 21.0285, lon: 105.8542}
    density_threshold: 30          # 30 xe = táº¯c
    video_source: "rtsp://camera-ip/stream"
    check_interval: 20             # Check má»—i 20s
    count_vehicle_types:
      - car
      - bus
      - motorcycle
      - truck
```

**Expected Behavior:**
- Normal: 10-20 vehicles
- Rush hour: 30-50 vehicles (trigger alert)
- Alert â†’ Backend táº¡o "outage" zone radius 200m

### 2. GiÃ¡m SÃ¡t Cao Tá»‘c

**Setup:**
```yaml
locations:
  - id: "sensor-highway-km15"
    name: "Cao Tá»‘c HÃ  Ná»™i - Háº£i PhÃ²ng KM15"
    density_threshold: 50          # 50 xe = Ã¹n táº¯c
    check_interval: 60             # Check má»—i phÃºt
    count_vehicle_types:
      - car
      - bus
      - container_truck
      - truck
```

**Expected Behavior:**
- High speed â†’ Low density
- Traffic jam â†’ High density
- Focus on large vehicles (trucks, buses)

### 3. Khu Vá»±c Ngáº­p Lá»¥t

**Setup:**
```yaml
locations:
  - id: "sensor-flood-prone-area"
    name: "ÄÆ°á»ng Nguyá»…n KhoÃ¡i (Khu Dá»… Ngáº­p)"
    density_threshold: 5           # Threshold tháº¥p = cáº£nh bÃ¡o sá»›m
    check_interval: 10             # Check thÆ°á»ng xuyÃªn
    count_vehicle_types:
      - car
      - motorcycle
```

**Expected Behavior:**
- Giáº£m Ä‘á»™t ngá»™t traffic â†’ CÃ³ thá»ƒ do ngáº­p
- Combine vá»›i water level sensors
- Alert â†’ Táº¡o "flood" zone

### 4. Parking Lot Monitoring

**Setup:**
```yaml
locations:
  - id: "sensor-parking-vincom"
    name: "BÃ£i Äá»— Xe Vincom"
    density_threshold: 80          # 80% capacity
    check_interval: 120            # Check má»—i 2 phÃºt
    count_vehicle_types:
      - car
      - motorcycle
```

**Expected Behavior:**
- Count parked vehicles
- Alert khi gáº§n full
- KhÃ´ng cáº§n táº¡o zones, chá»‰ thÃ´ng bÃ¡o

---

## ğŸ” Báº¢O Máº¬T & BEST PRACTICES

### Security Considerations

**1. API Credentials**
```yaml
# DON'T commit sensitive configs to git
api:
  endpoint: "${API_ENDPOINT}"     # Use env vars
  api_key: "${API_KEY}"           # If needed
```

**2. Camera Access**
- Use RTSP authentication
- Restrict network access to cameras
- Monitor unauthorized access attempts

**3. Data Privacy**
- KhÃ´ng save áº£nh cÃ³ máº·t ngÆ°á»i rÃµ rÃ ng
- Blur faces náº¿u cáº§n (OpenCV)
- Follow GDPR/data privacy laws
- Auto-delete old detection images

### Production Best Practices

**1. Monitoring & Alerting**
- Setup log aggregation (ELK stack)
- Monitor process health
- Alert on high error rates
- Track inference latency

**2. Reliability**
```python
# Auto-restart on crash
while True:
    try:
        monitor = TrafficDensityMonitor(config)
        monitor.run()
    except Exception as e:
        logger.error(f"Crashed: {e}")
        time.sleep(10)  # Wait before restart
```

**3. Resource Management**
- Limit max_det to prevent memory spike
- Rotate logs (logrotate)
- Clean old detection images
- Monitor disk usage

**4. Configuration Management**
- Use environment-specific configs
- Validate config on startup
- Document all parameters
- Version control configs (git)

---

## ğŸ“Š LUá»’NG Dá»® LIá»†U Tá»”NG QUÃT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Camera/Video Feed  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenCV Capture    â”‚
â”‚   Read Frame        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO Model        â”‚
â”‚   - Detect objects  â”‚
â”‚   - Classify        â”‚
â”‚   - Get bboxes      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vehicle Counter    â”‚
â”‚  - Count by type    â”‚
â”‚  - Sum total        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
      [Threshold?]
           â”‚
     YES â”€â”€â”´â”€â”€ NO â†’ Continue
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Save Detection     â”‚
â”‚  Image (optional)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Client         â”‚
â”‚  POST sensor-data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend System     â”‚
â”‚  - Check rules      â”‚
â”‚  - Auto create zonesâ”‚
â”‚  - WebSocket notify â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend Map       â”‚
â”‚  Display zones      â”‚
â”‚  Real-time update   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ TÃ€I LIá»†U THAM KHáº¢O

### YOLO & Ultralytics
- Ultralytics Docs: https://docs.ultralytics.com/
- YOLO v8 Paper: https://arxiv.org/abs/2305.09972
- YOLO v11 Release: https://github.com/ultralytics/ultralytics
- Training Guide: https://docs.ultralytics.com/modes/train/

### Computer Vision
- OpenCV Docs: https://docs.opencv.org/
- NumPy Docs: https://numpy.org/doc/

### Dataset
- Roboflow Universe: https://universe.roboflow.com/
- Dataset URL: https://universe.roboflow.com/luong-duc/vehicle_detection_project-8jikm/dataset/1
- YOLO Format: https://docs.ultralytics.com/datasets/detect/

### Integration
- Requests Docs: https://requests.readthedocs.io/
- PyYAML: https://pyyaml.org/

---

## ğŸ”® HÆ¯á»šNG PHÃT TRIá»‚N

### Phase 1: Current Features âœ…
- âœ… YOLO detection (8 vehicle classes)
- âœ… Real-time monitoring
- âœ… Threshold-based alerting
- âœ… API integration
- âœ… Multi-location support

### Phase 2: Enhanced Detection
- [ ] Vehicle tracking (track IDs, trajectories)
- [ ] Speed estimation
- [ ] Traffic flow analysis (in/out counting)
- [ ] License plate recognition (OCR)
- [ ] Vehicle color detection

### Phase 3: Advanced Analytics
- [ ] Historical trend analysis
- [ ] Predictive modeling (ML)
- [ ] Anomaly detection
- [ ] Traffic pattern recognition
- [ ] Peak hour analysis

### Phase 4: Scalability
- [ ] Distributed processing (multi-GPU)
- [ ] Cloud deployment (AWS, Azure)
- [ ] Edge device support (Jetson Nano, Raspberry Pi)
- [ ] Kubernetes orchestration
- [ ] Load balancing cho nhiá»u cameras

### Phase 5: Integration
- [ ] MQTT protocol support
- [ ] Kafka streaming
- [ ] Time-series database (InfluxDB)
- [ ] Grafana dashboards
- [ ] Mobile notifications

### Phase 6: AI Improvements
- [ ] Active learning (improve model vá»›i production data)
- [ ] Multi-object tracking (MOT)
- [ ] 3D detection (depth estimation)
- [ ] Night/bad weather model variants
- [ ] Transformer-based models (DETR, ViT)

---

## ğŸ“ SUPPORT & MAINTENANCE

### Logging & Debugging

**Enable Debug Logging:**
```yaml
logging:
  level: "DEBUG"
```

**Check Logs:**
```bash
# Real-time
tail -f monitoring.log

# Search errors
grep "ERROR" monitoring.log

# Count alerts
grep "HIGH DENSITY ALERT" monitoring.log | wc -l
```

### System Requirements Check

```bash
# Check Python version
python --version  # Should be 3.8+

# Check CUDA (GPU)
nvidia-smi

# Check OpenCV
python -c "import cv2; print(cv2.__version__)"

# Check Ultralytics
python -c "from ultralytics import YOLO; print('OK')"
```

### Monitoring Health

```bash
# CPU usage
top -p $(pgrep -f traffic_monitor)

# GPU usage
nvidia-smi -l 1

# Disk usage
df -h ./detections

# Memory usage
free -h
```

---

## ğŸ‘¥ TEAM & LICENSE

- **Maintainer**: PKA-OpenLD
- **License**: Apache License 2.0
- **Dataset License**: CC BY 4.0
- **Year**: 2025
- **Contact**: GitHub Issues

---

_TÃ i liá»‡u nÃ y Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng dá»±a trÃªn phÃ¢n tÃ­ch source code._
_Cáº­p nháº­t láº§n cuá»‘i: 2025-12-05_
